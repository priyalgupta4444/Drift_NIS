# -*- coding: utf-8 -*-
"""CIC-IDS2017.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kQPKFxft7_hfnfn1baC2cWQM4HjXBIiK

# Pre processing


---
"""

import pandas as pd
import os

from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

csv_paths = [
    '/content/drive/MyDrive/Colab Data/TrafficLabelling/File1.csv',
    '/content/drive/MyDrive/Colab Data/TrafficLabelling/File2.csv',
    '/content/drive/MyDrive/Colab Data/TrafficLabelling/File3.csv',
    '/content/drive/MyDrive/Colab Data/TrafficLabelling/File4.csv',
    '/content/drive/MyDrive/Colab Data/TrafficLabelling/File5.csv',
    '/content/drive/MyDrive/Colab Data/TrafficLabelling/File6.csv',
    '/content/drive/MyDrive/Colab Data/TrafficLabelling/File7.csv',
    '/content/drive/MyDrive/Colab Data/TrafficLabelling/File8.csv',
]

# Load and concatenate
dfs = [pd.read_csv(path, encoding='latin-1') for path in csv_paths] # Try 'latin-1' encoding first
# If 'latin-1' doesn't work, try other encodings like 'cp1252', 'iso-8859-1', etc.
full_df = pd.concat(dfs, ignore_index=True)

print("✅ Combined DataFrame shape:", full_df.shape)

# This modifies full_df in-place
full_df.drop(full_df[full_df[' Label'] == 'BENIGN'].index, inplace=True)

# Check the shape after deletion
print("Updated shape after removing 'Benign':", full_df.shape)

# Keep only 50% of the data randomly
reduced_df = full_df.sample(frac=0.25, random_state=42)

# Optional: Free up RAM
del full_df
import gc
gc.collect()

print("✅ Reduced DataFrame shape:", reduced_df.shape)
full_df = reduced_df

# Step 1: Save the DataFrame to a CSV file
full_df.to_csv("50_down.csv", index=False)

# Step 2: Use Colab's files module to download it
from google.colab import files
files.download("50_down.csv")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

print(full_df.info())           # Check types
print(full_df.isnull().sum())   # Missing values

# Drop unnecessary columns (like 'id', 'timestamp' if present)
full_df = full_df.drop(columns=['timestamp'], errors='ignore')  # Modify if needed

# Drop rows with missing target
# Check if 'Label' column exists before dropping rows
if ' Label' in full_df.columns:
    full_df = full_df.dropna(subset=[" Label"])
else:
    # Print a warning or handle the case where 'Label' column is missing
    print("Warning: 'Label' column not found in the DataFrame. Skipping dropna.")
    # You might need to identify the correct target column name
    # and update the code accordingly.

# Encode the target if it's categorical (e.g., 'Normal', 'Attack')
# Check if 'Label' column exists before encoding
if ' Label' in full_df.columns:
    full_df[" Label"] = full_df[" Label"].astype("category").cat.codes

print(full_df.columns.tolist())

X = full_df.drop(" Label", axis=1)
y = full_df[" Label"]

# Remove non-numeric columns if any
X = X.select_dtypes(include=[np.number])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

import numpy as np

# Split test set into 2 chunks
mid_point = len(X_test) // 2
X_test_1, y_test_1 = X_test[:mid_point], y_test[:mid_point]
X_test_2, y_test_2 = X_test[mid_point:], y_test[mid_point:]

# Apply transformation to simulate drift
X_test_2_drifted = X_test_2.copy()
X_test_2_drifted += np.random.normal(loc=0.5, scale=0.1, size=X_test_2.shape)  # add noise

# Concatenate to form full drifted test set
X_test_drifted = np.vstack([X_test_1, X_test_2_drifted])
y_test_drifted = np.hstack([y_test_1, y_test_2])

X_test = X_test_drifted
y_test = y_test_drifted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# ... (your existing code) ...

# Before scaling, replace infinite values with NaN in both X_train and X_test
X_train = X_train.replace([np.inf, -np.inf], np.nan)
# Convert X_test to DataFrame before applying replace
X_test = pd.DataFrame(X_test).replace([np.inf, -np.inf], np.nan)  # Handle X_test as well

# Impute NaN values using the median (or another strategy) for both
X_train = X_train.fillna(X_train.median())
X_test = X_test.fillna(X_test.median())  # Handle X_test as well

# Now you can scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
#Convert X_test back to numpy array before scaling
X_test_scaled = scaler.transform(X_test.to_numpy())

# ... (rest of your code) ...

y_train = y_train.replace([np.inf, -np.inf], np.nan)
# Convert y_test to Pandas Series before using replace
y_test = pd.Series(y_test).replace([np.inf, -np.inf], np.nan)

# Impute NaN values using the median (or another strategy) for both
y_train = y_train.fillna(y_train.median())
y_test = y_test.fillna(y_test.median())  # Handle y_test as well



from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_scaled, y_train)  # Make sure this is already run before evaluating!

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Define batch size
batch_size = 2500  # Adjust based on RAM
n_batches = int(np.ceil(X_test_scaled.shape[0] / batch_size))

# Store metrics
batch_metrics = []
acc_rf = []

# Predict and evaluate in batches using drifted labels
for i in range(n_batches):
    start = i * batch_size
    end = min((i + 1) * batch_size, X_test_scaled.shape[0])

    X_batch = X_test_scaled[start:end]
    y_batch = y_test[start:end]  # <-- Use drifted labels here

    y_pred = clf.predict(X_batch)

    acc = accuracy_score(y_batch, y_pred)
    prec = precision_score(y_batch, y_pred, average='macro', zero_division=1)
    rec = recall_score(y_batch, y_pred, average='macro', zero_division=1)
    f1 = f1_score(y_batch, y_pred, average='macro', zero_division=1)

    batch_metrics.append([acc, prec, rec, f1])
    acc_rf.append(acc)
    print(f"📦 Batch {i+1}/{n_batches} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

# Convert to NumPy array and compute mean
batch_metrics = np.array(batch_metrics)
avg_acc, avg_prec, avg_rec, avg_f1 = np.mean(batch_metrics, axis=0)

print("\n📊 Final Average Metrics across all batches (with Drifted Labels):")
print(f"✅ Accuracy: {avg_acc:.4f}")
print(f"✅ Precision: {avg_prec:.4f}")
print(f"✅ Recall: {avg_rec:.4f}")
print(f"✅ F1-score: {avg_f1:.4f}")

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.naive_bayes import GaussianNB

# Train Naive Bayes (if not already trained)
clf_nb = GaussianNB()
clf_nb.fit(X_train_scaled, y_train)

# Define batch size
batch_size = 2500  # Adjust based on your RAM
n_batches = int(np.ceil(X_test_scaled.shape[0] / batch_size))

# Store metrics
batch_metrics_nb = []
acc_nb =[]
# Predict and evaluate in batches
for i in range(n_batches):
    start = i * batch_size
    end = min((i + 1) * batch_size, X_test_scaled.shape[0])

    X_batch = X_test_scaled[start:end]
    y_batch = y_test_drifted[start:end]

    y_pred = clf_nb.predict(X_batch)

    acc = accuracy_score(y_batch, y_pred)
    prec = precision_score(y_batch, y_pred, average='macro', zero_division=1)
    rec = recall_score(y_batch, y_pred, average='macro', zero_division=1)
    f1 = f1_score(y_batch, y_pred, average='macro', zero_division=1)

    batch_metrics_nb.append([acc, prec, rec, f1])
    acc_nb.append(acc)
    print(f"📦 NB Batch {i+1}/{n_batches} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

# Convert to NumPy array and compute mean
batch_metrics_nb = np.array(batch_metrics_nb)
avg_acc, avg_prec, avg_rec, avg_f1 = np.mean(batch_metrics_nb, axis=0)

print("\n📊 Naive Bayes - Final Average Metrics across all batches:")
print(f"✅ Accuracy: {avg_acc:.4f}")
print(f"✅ Precision: {avg_prec:.4f}")
print(f"✅ Recall: {avg_rec:.4f}")
print(f"✅ F1-score: {avg_f1:.4f}")

"""

---

# KL -DIVERGENCE"""

import numpy as np
import pandas as pd
from scipy.stats import entropy
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# --------- KL Divergence Function ---------
def kl_divergence(p, q):
    p = np.histogram(p, bins=50, density=True)[0] + 1e-6
    q = np.histogram(q, bins=50, density=True)[0] + 1e-6
    return entropy(p, q)

# --------- Parameters ---------
threshold = 18.0
batch_size = 2500
retrain_window = 5
scaler = StandardScaler()

# --------- Scale Training Data ---------
X_train_scaled = scaler.fit_transform(X_train)

# --------- Train Initial Random Forest ---------
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Ensure y_test is a Series
if isinstance(y_test, np.ndarray):
    y_test = pd.Series(y_test)

batch_results = []
acc_kl_rf = []
recent_X = []
recent_y = []

# --------- Batch-wise Evaluation Loop ---------
for batch_start in range(0, X_test.shape[0], batch_size):
    X_batch = X_test.iloc[batch_start:batch_start + batch_size].copy()
    y_batch = y_test.iloc[batch_start:batch_start + batch_size].copy().to_numpy()

    # Drop rows with NaNs to ensure consistency in shape
    X_batch = X_batch.dropna()

    # Skip batch if sizes don't match due to dropped rows
    if X_batch.shape[0] != batch_size:
        print(f"⚠️ Skipping Batch {batch_start // batch_size + 1} due to NaNs or row mismatch.")
        continue

    # Scale the batch data
    X_batch_scaled = scaler.transform(X_batch)

    # --------- KL Divergence Drift Detection ---------
    kl_values = [kl_divergence(X_train_scaled[:, i], X_batch_scaled[:, i]) for i in range(X_train_scaled.shape[1])]
    mean_kl = np.mean(kl_values)
    max_kl = np.max(kl_values)
    is_drift = max_kl > threshold

    # --------- Model Evaluation ---------
    preds = model.predict(X_batch_scaled)
    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1 = f1_score(y_batch, preds, average="macro", zero_division=1)

    acc_kl_rf.append(acc)
    batch_results.append({
        "Batch": batch_start // batch_size + 1,
        "Model": "Random Forest",
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1,
        "Drift": is_drift
    })

    print(f"🌲 Batch {batch_start // batch_size + 1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f} | Drift: {'Yes' if is_drift else 'No'} | Mean KL: {mean_kl:.2f} | Max KL: {max_kl:.2f}")

    # --------- Maintain Recent History ---------
    recent_X.append(X_batch)
    recent_y.append(y_batch)
    recent_X = recent_X[-retrain_window:]
    recent_y = recent_y[-retrain_window:]

    # --------- Retrain if Drift Detected ---------
    if is_drift:
        X_retrain = np.vstack([X_train] + recent_X)
        y_retrain = np.hstack([y_train] + recent_y)

        unique_classes, class_counts = np.unique(y_retrain, return_counts=True)
        if len(unique_classes) > 1 and min(class_counts) > 5:
            smote = SMOTE(k_neighbors=min(5, min(class_counts) - 1))
            X_resampled, y_resampled = smote.fit_resample(X_retrain, y_retrain)
            X_resampled_scaled = scaler.fit_transform(X_resampled)
            model.fit(X_resampled_scaled, y_resampled)
            print(f"🔁 Model retrained on combined X_train + last {retrain_window} batches.")
        else:
            print(f"⚠️ Skipping SMOTE due to class imbalance.")

# --------- Summary ---------
results_df = pd.DataFrame(batch_results)

print("\n📊 Average Metrics Across All Batches (Random Forest + KL Drift):")
print(results_df.groupby("Model")[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

print("\n📈 Drift Summary:")
print(results_df["Drift"].value_counts())

import numpy as np
import pandas as pd
from scipy.stats import entropy
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# --------- KL Divergence Function ---------
def kl_divergence(p, q):
    p = np.histogram(p, bins=50, density=True)[0] + 1e-6
    q = np.histogram(q, bins=50, density=True)[0] + 1e-6
    return entropy(p, q)

# --------- Parameters ---------
threshold = 18.0
batch_size = 2500
retrain_window = 5
scaler = StandardScaler()

# --------- Fit scaler ONLY on X_train ---------
X_train_scaled = scaler.fit_transform(X_train)

# Train initial Naive Bayes model
model = GaussianNB()
model.fit(X_train_scaled, y_train)

# Ensure y_test is a Series
if isinstance(y_test, np.ndarray):
    y_test = pd.Series(y_test)

batch_results = []
acc_kl_nb = []
recent_X = []
recent_y = []

# --------- Main Loop ---------
for batch_start in range(0, X_test.shape[0], batch_size):
    X_batch = X_test.iloc[batch_start:batch_start + batch_size].copy()
    y_batch = y_test.iloc[batch_start:batch_start + batch_size].copy().to_numpy()

    # Drop rows with NaNs to ensure consistency in shape
    X_batch = X_batch.dropna()

    # Skip batch if sizes don't match due to dropped rows
    if X_batch.shape[0] != batch_size:
        print(f"⚠️ Skipping Batch {batch_start // batch_size + 1} due to NaNs or row mismatch.")
        continue

    # Transform batch using pre-fit scaler
    X_batch_scaled = scaler.transform(X_batch)

    # --------- KL Divergence Drift Detection ---------
    kl_values = [kl_divergence(X_train_scaled[:, i], X_batch_scaled[:, i]) for i in range(X_train_scaled.shape[1])]
    mean_kl = np.mean(kl_values)
    max_kl = np.max(kl_values)

    # --------- Evaluate Pretrained Model ---------
    predictions = model.predict(X_batch_scaled)
    acc = accuracy_score(y_batch, predictions)
    prec = precision_score(y_batch, predictions, average="macro", zero_division=1)
    rec = recall_score(y_batch, predictions, average="macro", zero_division=1)
    f1 = f1_score(y_batch, predictions, average="macro", zero_division=1)

    is_drift = max_kl > threshold
    acc_kl_nb.append(acc)

    batch_results.append({
        "Batch": batch_start // batch_size + 1,
        "Model": "Naive Bayes",
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1,
        "Drift": is_drift
    })

    print(f"📦 Batch {batch_start // batch_size + 1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f} | Drift: {'Yes' if is_drift else 'No'} | Mean KL: {mean_kl:.2f} | Max KL: {max_kl:.2f}")

    # Store recent batches
    recent_X.append(X_batch)
    recent_y.append(y_batch)
    recent_X = recent_X[-retrain_window:]
    recent_y = recent_y[-retrain_window:]

    # --------- Retrain if Drift Detected ---------
    if is_drift:
        X_retrain = np.vstack([X_train] + recent_X)
        y_retrain = np.hstack([y_train] + recent_y)

        unique_classes, class_counts = np.unique(y_retrain, return_counts=True)
        if len(unique_classes) > 1 and min(class_counts) > 5:
            smote = SMOTE(k_neighbors=min(5, min(class_counts) - 1))
            X_resampled, y_resampled = smote.fit_resample(X_retrain, y_retrain)
            X_resampled_scaled = scaler.fit_transform(X_resampled)
            model.fit(X_resampled_scaled, y_resampled)
            print(f"🔁 Model retrained on combined X_train + last {retrain_window} batches.")
        else:
            print(f"⚠️ Skipping SMOTE due to class imbalance.")

# --------- Summary ---------
results_df = pd.DataFrame(batch_results)

print("\n📊 Average Metrics Across All Batches (Naive Bayes + KL Drift):")
print(results_df.groupby("Model")[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

print("\n📈 Drift Summary:")
print(results_df["Drift"].value_counts())

"""

---

# Accuracy based sliding window"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import pandas as pd

# --------- Parameters ---------
drift_threshold = 0.75
retrain_window = 5
batch_size = 2500  # or your desired batch size

# --------- Reinitialize Variables ---------
batch_metrics = []
acc_sl_nb = []
recent_X = []
recent_y = []

# --------- Train Initial Model ---------
clf = GaussianNB()
clf.fit(X_train_scaled, y_train)

# --------- Batch-wise Evaluation ---------
for batch_start in range(0, X_test.shape[0], batch_size):
    batch_end = batch_start + batch_size
    X_batch = X_test.iloc[batch_start:batch_end].copy()
    y_batch = y_test.iloc[batch_start:batch_end].copy().to_numpy()

    # Drop rows with NaNs to ensure consistency in shape
    X_batch = X_batch.dropna()

    # Skip batch if sizes don't match due to dropped rows
    if X_batch.shape[0] != batch_size:
        print(f"⚠️ Skipping Batch {batch_start // batch_size + 1} due to NaNs or row mismatch.")
        continue

    # Scale the batch using previously fit scaler
    X_batch_scaled = scaler.transform(X_batch)

    preds = clf.predict(X_batch_scaled)

    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1 = f1_score(y_batch, preds, average="macro", zero_division=1)

    acc_sl_nb.append(acc)
    batch_metrics.append({
        "Batch": batch_start // batch_size + 1,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    print(f"📦 Batch {batch_start // batch_size + 1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

    # Save for potential retraining
    recent_X.append(X_batch_scaled)
    recent_y.append(y_batch)
    recent_X = recent_X[-retrain_window:]
    recent_y = recent_y[-retrain_window:]

    # --------- Drift Detection & Retraining ---------
    if acc < drift_threshold:
        print(f"⚠️ Drift detected at batch {batch_start // batch_size + 1}! Retraining model...")
        X_retrain = np.vstack([X_train_scaled] + recent_X)
        y_retrain = np.hstack([y_train] + recent_y)
        clf.fit(X_retrain, y_retrain)
        print(f"🔁 Model retrained on training + last {retrain_window} batches.")

# --------- Summary ---------
results_df = pd.DataFrame(batch_metrics)

print("\n📊 Average Metrics Across All Batches (Naive Bayes + Accuracy Threshold):")
print(results_df[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import pandas as pd

# --------- Parameters ---------
drift_threshold = 0.60
retrain_window = 3
batch_size = 2500  # Adjust if needed

# --------- Train Initial Model ---------
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_scaled, y_train)

# --------- Drift Injection (run this once before batching) ---------
y_test_drifted = y_test.to_numpy().copy()
for i in range(len(y_test_drifted)//2, len(y_test_drifted)):
    if np.random.rand() < 0.2:
        current = y_test_drifted[i]
        choices = [lbl for lbl in np.unique(y_test_drifted) if lbl != current]
        y_test_drifted[i] = np.random.choice(choices)

# --------- Variables to Track ---------
batch_metrics = []
recent_X = []
recent_y = []
drifted_X = []
drifted_y = []

# --------- Batch-wise Evaluation & Adaptive Retraining ---------
for batch_start in range(0, X_test.shape[0], batch_size):
    batch_end = batch_start + batch_size

    # 1) Slice features & (drifted) labels
    X_batch = X_test.iloc[batch_start:batch_end].dropna()
    y_batch = y_test_drifted[batch_start:batch_end]  # already numpy

    # 2) Skip if size mismatch
    if X_batch.shape[0] != batch_size:
        print(f"⚠️ Skipping batch {batch_start//batch_size+1}: size {X_batch.shape[0]}")
        continue

    # 3) Scale & predict
    Xb_scaled = scaler.transform(X_batch)
    preds = clf.predict(Xb_scaled)

    # 4) Compute metrics
    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1  = f1_score(y_batch, preds, average="macro", zero_division=1)

    batch_metrics.append({
        "Batch": batch_start//batch_size+1,
        "Accuracy": acc, "Precision": prec,
        "Recall": rec, "F1-score": f1
    })
    print(f"📦 Batch {batch_start//batch_size+1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

    # 5) Store for retraining
    recent_X.append(Xb_scaled)
    recent_y.append(y_batch)
    recent_X = recent_X[-retrain_window:]
    recent_y = recent_y[-retrain_window:]

    # 6) Check drift & retrain if needed
    if acc < drift_threshold:
        print(f"⚠️ Drift detected at batch {batch_start//batch_size+1}! Retraining…")
        drifted_X.append(Xb_scaled)
        drifted_y.append(y_batch)

        X_retrain = np.vstack([X_train_scaled] + recent_X + drifted_X)
        y_retrain = np.hstack([y_train] + recent_y + drifted_y)

        clf.fit(X_retrain, y_retrain)
        print("🔁 Model retrained on train + recent + drifted data.")

# --------- Final Summary ---------
results_df = pd.DataFrame(batch_metrics)
print("\n📊 Final Average Metrics:")
print(results_df[["Accuracy","Precision","Recall","F1-score"]].mean().round(4))

"""

---
# eddm
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# --------- Train initial Random Forest model ---------
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_scaled, y_train)

# --------- EDDM Class ---------
class SimpleEDDM:
    def __init__(self, min_errors=10, drift_threshold=1):
        self.min_errors = min_errors
        self.drift_threshold = drift_threshold
        self.reset()

    def reset(self):
        self.error_indices = []
        self.last_index = -1

    def update(self, predictions, truths, batch_index):
        errors = np.where(predictions != truths)[0]
        if len(errors) == 0:
            return False

        for err in errors:
            if self.last_index != -1:
                distance = err - self.last_index
                self.error_indices.append(distance)
            self.last_index = err

        if len(self.error_indices) < self.min_errors:
            return False

        mean_dist = np.mean(self.error_indices)
        std_dist = np.std(self.error_indices)
        score = mean_dist + 2 * std_dist

        if score < self.drift_threshold:
            print(f"⚠️ EDDM Drift detected at batch {batch_index+1} (score: {score:.4f})")
            self.reset()
            return True
        return False

# --------- Split into batches ---------
def split_into_batches(X, y, batch_size):
    for i in range(0, len(X), batch_size):
        yield X[i:i + batch_size], y[i:i + batch_size]

# --------- Parameters ---------
batch_size = 2500
batches = list(split_into_batches(X_test_scaled, y_test, batch_size))
eddm = SimpleEDDM()
recent_X = []
recent_y = []
retrain_window = 5
acc_eddm_rf = []
batch_metrics = []

# --------- Main Loop ---------
for i, (X_batch, y_batch) in enumerate(batches):
    preds = clf.predict(X_batch)

    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1 = f1_score(y_batch, preds, average="macro", zero_division=1)
    acc_eddm_rf.append(acc)

    batch_metrics.append({
        "Batch": i + 1,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    print(f"📦 Batch {i+1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

    recent_X.append(X_batch)
    recent_y.append(y_batch)
    recent_X = recent_X[-retrain_window:]
    recent_y = recent_y[-retrain_window:]

    # --------- Drift Detection & Retraining ---------
    if eddm.update(preds, y_batch, i):
        X_retrain = np.vstack([X_train_scaled, *recent_X])
        y_retrain = np.hstack([y_train, *recent_y])
        clf.fit(X_retrain, y_retrain)
        print(f"🔁 Model retrained on training + last {retrain_window} batches.")

# --------- Final Summary ---------
results_df = pd.DataFrame(batch_metrics)
print("\n📊 Average Metrics Across All Batches (Random Forest with EDDM):")
print(results_df[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# --------- EDDM Class ---------
class SimpleEDDM:
    def __init__(self, min_errors=10, drift_threshold=10):
        self.min_errors = min_errors
        self.drift_threshold = drift_threshold
        self.reset()

    def reset(self):
        self.error_indices = []
        self.last_index = -1

    def update(self, predictions, truths, batch_index):
        errors = np.where(predictions != truths)[0]
        if len(errors) == 0:
            return False
        for err in errors:
            if self.last_index != -1:
                self.error_indices.append(err - self.last_index)
            self.last_index = err
        if len(self.error_indices) < self.min_errors:
            return False
        score = np.mean(self.error_indices) + 2 * np.std(self.error_indices)
        if score < self.drift_threshold:
            print(f"⚠️ EDDM Drift detected at batch {batch_index+1} (score: {score:.4f})")
            self.reset()
            return True
        return False

# --------- Drift Injection ---------
# Make a copy of y_test as a NumPy array
y_test_drifted = y_test.to_numpy().copy()

# Flip 20% of the labels in the second half
for i in range(len(y_test_drifted)//2, len(y_test_drifted)):
    if np.random.rand() < 0.2:
        current = y_test_drifted[i]
        choices = [lbl for lbl in np.unique(y_test_drifted) if lbl != current]
        y_test_drifted[i] = np.random.choice(choices)

# --------- Split into batches helper ---------
def split_into_batches(X, y, batch_size):
    for i in range(0, len(X), batch_size):
        yield X[i:i + batch_size], y[i:i + batch_size]

# --------- Parameters ---------
batch_size = 2500
batches = list(split_into_batches(X_test_scaled, y_test_drifted, batch_size))
eddm = SimpleEDDM(min_errors=5, drift_threshold=1)
recent_X = []
recent_y = []
retrain_window = 5
batch_metrics = []

# --------- Initial Training ---------
clf = GaussianNB()
clf.fit(X_train_scaled, y_train)

# --------- Main Loop ---------
for i, (X_batch, y_batch) in enumerate(batches):
    # 1) Predict & measure
    preds = clf.predict(X_batch)
    acc  = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec  = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1   = f1_score(y_batch, preds, average="macro", zero_division=1)

    batch_metrics.append({
        "Batch": i+1, "Accuracy": acc,
        "Precision": prec, "Recall": rec, "F1-score": f1
    })
    print(f"📦 Batch {i+1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

    # 2) Save for retraining
    recent_X.append(X_batch)
    recent_y.append(y_batch)
    recent_X = recent_X[-retrain_window:]
    recent_y = recent_y[-retrain_window:]

    # 3) Drift check & retrain
    if eddm.update(preds, y_batch, i):
        X_retrain = np.vstack([X_train_scaled] + recent_X)
        y_retrain = np.hstack([y_train] + recent_y)
        clf.fit(X_retrain, y_retrain)
        print(f"🔁 Model retrained on train + last {retrain_window} batches.")

# --------- Summary ---------
results_df = pd.DataFrame(batch_metrics)
print("\n📊 Average Metrics Across All Batches (Naive Bayes + EDDM with Drift):")
print(results_df[["Accuracy","Precision","Recall","F1-score"]].mean().round(4))

"""

---

# Graphs"""

import matplotlib.pyplot as plt
import numpy as np

# Methods and metrics
methods = ['Without Drift Handling', 'KL-Divergence', 'Accuracy-Based Window', 'EDDM']
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']

# Values (as given)
values = [
    [0.8941,0.8988,0.5361,0.5741],  # No Drift Handling
    [0.9853,0.9657,0.6843,0.7183],  # KL-Divergence
    [0.9762,0.9589,0.6951,0.7265],  # Sliding Window
    [0.9866,0.9671,0.6934,0.7168]   # EDDM
]

# Transpose for plotting (metric-wise grouping)
values = np.array(values).T

# Plotting
x = np.arange(len(metrics))  # Metric categories
bar_width = 0.2

plt.figure(figsize=(10, 6))

for i in range(len(methods)):
    plt.bar(x + i * bar_width, values[:, i], width=bar_width, label=methods[i])

# Labels and formatting
plt.xlabel('Evaluation Metric')
plt.ylabel('Score')
plt.title('Comparison of Drift Detection Methods')
plt.xticks(x + bar_width * 1.5, metrics)
plt.ylim(0, 1)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Methods and metrics
methods = ['Without Drift Handling', 'KL-Divergence', 'Accuracy-Based Window', 'EDDM']
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']

# Values (as given)
values = [
    [0.7898,0.7053,0.4853,0.4679],  # No Drift Handling
    [0.8691,0.7533,0.6126,0.5602],  # KL-Divergence
    [0.8715,0.7681,0.6048,0.5906],  # Sliding Window
    [0.8726,0.7121,0.6048,0.5784]   # EDDM
]

# Transpose for plotting (metric-wise grouping)
values = np.array(values).T

# Plotting
x = np.arange(len(metrics))  # Metric categories
bar_width = 0.2

plt.figure(figsize=(10, 6))

for i in range(len(methods)):
    plt.bar(x + i * bar_width, values[:, i], width=bar_width, label=methods[i])

# Labels and formatting
plt.xlabel('Evaluation Metric')
plt.ylabel('Score')
plt.title('Comparison of Drift Detection Methods')
plt.xticks(x + bar_width * 1.5, metrics)
plt.ylim(0, 1)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()