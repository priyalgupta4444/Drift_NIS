# -*- coding: utf-8 -*-
"""UNSW-N1B5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vx6VJQRzJdErmUvIn6ZvDOW6LOSh4fuS

---


# Preprocessing



---
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

!pip install tensorflow==1.15
import tensorflow as tf

from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np

# Load dataset from Google Drive
file_path = "/content/drive/My Drive/Colab Data/UNSW_NB15_training-set.csv"
train = pd.read_csv(file_path)

file_path = "/content/drive/My Drive/Colab Data/UNSW_NB15_testing-set.csv"
test = pd.read_csv(file_path)

csv_columns = ['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label']
attack_cat = ['Normal','Fuzzers','Analysis','Backdoor','Exploit','Generic','Reconnaissance','Shellcode','Worms']

#train = pd.read_csv('UNSW_NB15_training-set.csv', names=csv_columns, header=None)
#test = pd.read_csv('UNSW_NB15_testing-set.csv', names=csv_columns, header=None)

train.head()

train_y = train.pop('attack_cat')
test_y = test.pop('attack_cat')
train.head()
train.drop(columns=['id'], inplace=True)
test.drop(columns=['id'], inplace=True)

train.shape

categorical_cols = train.select_dtypes(include=['object']).columns.tolist()

label_encoders = {}

for col in categorical_cols:
    train[col].fillna("Missing", inplace=True)  # Replace NaN with a placeholder
    test[col].fillna("Missing", inplace=True)

    train[col] = train[col].astype(str)  # Convert to string
    test[col] = test[col].astype(str)

    le = LabelEncoder()

    # Fit on the combined unique values from both train and test
    all_values = pd.concat([train[col], test[col]], ignore_index=True).unique()
    le.fit(all_values)

    train[col] = le.transform(train[col])
    test[col] = le.transform(test[col])
    label_encoders[col] = le

# Ensure the target variable is encoded too
le_target = LabelEncoder()
train_y = le_target.fit_transform(train_y)
test_y = le_target.transform(test_y)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
train_scaled = scaler.fit_transform(train)
test_scaled = scaler.transform(test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

rf_classifier.fit(train, train_y)

predictions = rf_classifier.predict(test)

accuracy = accuracy_score(test_y, predictions)
print(f"Accuracy: {accuracy:.4f}")

print(classification_report(test_y, predictions, target_names=le_target.classes_))

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import numpy as np

# Set your batch size
batch_size = 2500

acc_nb =[]

# Initialize the model
nb_classifier = GaussianNB()

# --------- Batch-wise Training ---------
for i in range(0, len(train), batch_size):
    batch_X = train[i:i+batch_size]
    batch_y = train_y[i:i+batch_size]

    # GaussianNB does not support partial_fit, so we simulate re-training on each batch
    nb_classifier.fit(batch_X, batch_y)

# --------- Batch-wise Prediction ---------
all_preds = []
for i in range(0, len(test), batch_size):
    batch_X = test[i:i+batch_size]
    preds = nb_classifier.predict(batch_X)
    all_preds.extend(preds)

# Truncate true labels to match prediction length
true_labels = test_y[:len(all_preds)]

# --------- Evaluation Metrics ---------
accuracy = accuracy_score(true_labels, all_preds)
precision = precision_score(true_labels, all_preds, average='weighted', zero_division=0)
recall = recall_score(true_labels, all_preds, average='weighted', zero_division=0)
f1 = f1_score(true_labels, all_preds, average='weighted', zero_division=0)
acc_nb.append(accuracy)
# --------- Output ---------
print(f"Naive Bayes Accuracy (batch-wise): {accuracy:.4f}")
print(f"Precision (weighted): {precision:.4f}")
print(f"Recall (weighted): {recall:.4f}")
print(f"F1 Score (weighted): {f1:.4f}")
print("\nClassification Report:")
print(classification_report(true_labels, all_preds, target_names=le_target.classes_))

import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Train your model on the full training data
nb_classifier = GaussianNB()
nb_classifier.fit(train, train_y)

# Set batch size
batch_size = 2500
batch_metrics = []
acc_no_drift = []

# Iterate through test data in batches
for start in range(0, len(test), batch_size):
    end = start + batch_size
    X_batch = test.iloc[start:end]
    y_batch = test_y[start:end]

    # Predict on batch
    preds = nb_classifier.predict(X_batch)

    # Calculate metrics
    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1 = f1_score(y_batch, preds, average="macro", zero_division=1)

    acc_no_drift.append(acc)
    batch_metrics.append({
        "Batch": start // batch_size + 1,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    print(f"📦 Batch {start // batch_size + 1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

# Convert to DataFrame for averaging
results_df = pd.DataFrame(batch_metrics)
print("\n📊 Average Metrics Across All Batches:")
print(results_df[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

from xgboost import XGBClassifier

xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb_model.fit(train, train_y)

xgb_predictions = xgb_model.predict(test)
print(classification_report(test_y, xgb_predictions, target_names=le_target.classes_, zero_division=1))

accuracy = accuracy_score(test_y, xgb_predictions)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import pandas as pd

# --- Train logistic regression on full training data ---
log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(train_scaled, train_y)

# --- Batch split helper ---
def split_batches(X, y, batch_size):
    for i in range(0, len(X), batch_size):
        yield X[i:i + batch_size], y[i:i + batch_size]

batch_size = 2500
batches = list(split_batches(test_scaled, test_y, batch_size))

# --- Track metrics for each batch ---
batch_metrics = []
acc_lr = []

for i, (X_batch, y_batch) in enumerate(batches):
    preds = log_reg.predict(X_batch)

    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average='macro', zero_division=1)
    rec = recall_score(y_batch, preds, average='macro', zero_division=1)
    f1 = f1_score(y_batch, preds, average='macro', zero_division=1)

    print(f"📦 Batch {i+1} | Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}")

    batch_metrics.append({
        "Batch": i + 1,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    acc_lr.append(acc)
# --- Final summary ---
results_df = pd.DataFrame(batch_metrics)
print("\n📊 Average Metrics Across All Batches (No Drift Handling):")
print(results_df[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

"""

---

# **EDDM**


---

"""

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(train_scaled, train_y)

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(train_scaled, train_y)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np

class SimpleEDDM:
    def __init__(self, min_errors=10, drift_threshold=13):
        self.min_errors = min_errors
        self.drift_threshold = drift_threshold
        self.reset()

    def reset(self):
        self.error_indices = []
        self.last_index = -1

    def update(self, predictions, truths, batch_index):
        errors = np.where(predictions != truths)[0]
        if len(errors) == 0:
            return False

        for err in errors:
            if self.last_index != -1:
                distance = err - self.last_index
                self.error_indices.append(distance)
            self.last_index = err

        if len(self.error_indices) < self.min_errors:
            return False

        mean_dist = np.mean(self.error_indices)
        std_dist = np.std(self.error_indices)
        score = mean_dist + 2 * std_dist

        if score < self.drift_threshold:
            print(f"⚠️ EDDM Drift detected at batch {batch_index+1} (score: {score:.4f})")
            self.reset()
            return True
        return False

# Helper: split test data
def split_into_batches(X, y, batch_size):
    for i in range(0, len(X), batch_size):
        yield X[i:i + batch_size], y[i:i + batch_size]

# Parameters
batch_size = 2500
batches = list(split_into_batches(test_scaled, test_y, batch_size))
eddm = SimpleEDDM()
recent_X = []
recent_y = []
retrain_window = 5
acc_eddm = []
# Metrics storage
batch_metrics = []

# Main loop
for i, (X_batch, y_batch) in enumerate(batches):
    preds = clf.predict(X_batch)

    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1 = f1_score(y_batch, preds, average="macro", zero_division=1)
    acc_eddm.append(acc)
    batch_metrics.append({
        "Batch": i + 1,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    print(f"📦 Batch {i+1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

    recent_X.append(X_batch)
    recent_y.append(y_batch)

    if eddm.update(preds, y_batch, i):
        X_retrain = np.vstack([train_scaled, *recent_X[-retrain_window:]])
        y_retrain = np.hstack([train_y, *recent_y[-retrain_window:]])
        clf.fit(X_retrain, y_retrain)
        print(f"🔁 Model retrained on last {retrain_window} batches.")

# Final Summary
results_df = pd.DataFrame(batch_metrics)
print("\n📊 Average Metrics Across All Batches:")
print(results_df[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Train initial Naive Bayes model
clf = GaussianNB()
clf.fit(train_scaled, train_y)

# EDDM class
class SimpleEDDM:
    def __init__(self, min_errors=10, drift_threshold=8):
        self.min_errors = min_errors
        self.drift_threshold = drift_threshold
        self.reset()

    def reset(self):
        self.error_indices = []
        self.last_index = -1

    def update(self, predictions, truths, batch_index):
        errors = np.where(predictions != truths)[0]
        if len(errors) == 0:
            return False

        for err in errors:
            if self.last_index != -1:
                distance = err - self.last_index
                self.error_indices.append(distance)
            self.last_index = err

        if len(self.error_indices) < self.min_errors:
            return False

        mean_dist = np.mean(self.error_indices)
        std_dist = np.std(self.error_indices)
        score = mean_dist + 2 * std_dist

        if score < self.drift_threshold:
            print(f"⚠️ EDDM Drift detected at batch {batch_index+1} (score: {score:.4f})")
            self.reset()
            return True
        return False

# Split into batches
def split_into_batches(X, y, batch_size):
    for i in range(0, len(X), batch_size):
        yield X[i:i + batch_size], y[i:i + batch_size]

# Parameters
batch_size = 2500
batches = list(split_into_batches(test_scaled, test_y, batch_size))
eddm = SimpleEDDM()
recent_X = []
recent_y = []
retrain_window = 5
acc_eddm_nb = []
batch_metrics = []

# Main Loop
for i, (X_batch, y_batch) in enumerate(batches):
    preds = clf.predict(X_batch)

    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1 = f1_score(y_batch, preds, average="macro", zero_division=1)
    acc_eddm_nb.append(acc)

    batch_metrics.append({
        "Batch": i + 1,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    print(f"📦 Batch {i+1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

    recent_X.append(X_batch)
    recent_y.append(y_batch)

    if eddm.update(preds, y_batch, i):
        X_retrain = np.vstack([train_scaled, *recent_X[-retrain_window:]])
        y_retrain = np.hstack([train_y, *recent_y[-retrain_window:]])
        clf.fit(X_retrain, y_retrain)
        print(f"🔁 Model retrained on last {retrain_window} batches.")

# Final Summary
results_df = pd.DataFrame(batch_metrics)
print("\n📊 Average Metrics Across All Batches (Naive Bayes with EDDM):")
print(results_df[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

"""

---

# **Drift sliding window method**


---

"""

def split_into_batches(X, y, batch_size):
    for i in range(0, len(X), batch_size):
        yield X[i:i+batch_size], y[i:i+batch_size]

batch_size = 2500
batches = list(split_into_batches(test_scaled, test_y, batch_size))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import pandas as pd

batch_metrics = []
acc_sl = []
recent_X = []
recent_y = []
drift_threshold = 0.68  # Tune as needed
retrain_window = 5      # Number of batches to use for retraining

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(train_scaled, train_y)  # Initial training

for i, (X_batch, y_batch) in enumerate(batches):
    preds = clf.predict(X_batch)

    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1 = f1_score(y_batch, preds, average="macro", zero_division=1)
    acc_sl.append(acc)
    batch_metrics.append({
        "Batch": i + 1,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    print(f"📦 Batch {i+1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

    recent_X.append(X_batch)
    recent_y.append(y_batch)

    if acc < drift_threshold:
        print(f"⚠️ Drift detected at batch {i+1}! Retraining the model...")
        X_retrain = np.vstack([train_scaled, *recent_X[-retrain_window:]])
        y_retrain = np.hstack([train_y, *recent_y[-retrain_window:]])
        clf.fit(X_retrain, y_retrain)
        print(f"🔁 Model retrained on last {retrain_window} batches.")

# Final summary
results_df = pd.DataFrame(batch_metrics)

print("\n📊 Average Metrics Across All Batches:")
print(results_df[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

overall_avg_accuracy = np.mean(batch_accuracies)
print(f"\n Overall Average Accuracy across all batches: {overall_avg_accuracy:.4f}")

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import pandas as pd

# Reinitialize variables
batch_metrics = []
acc_sl_nb = []
recent_X = []
recent_y = []
drift_threshold = 0.60  # You can tune this
retrain_window = 3      # Last N batches to use for retraining

# Train initial Naive Bayes model
clf = GaussianNB()
clf.fit(train_scaled, train_y)

# Assuming 'batches' is already defined using your previous split function
for i, (X_batch, y_batch) in enumerate(batches):
    preds = clf.predict(X_batch)

    acc = accuracy_score(y_batch, preds)
    prec = precision_score(y_batch, preds, average="macro", zero_division=1)
    rec = recall_score(y_batch, preds, average="macro", zero_division=1)
    f1 = f1_score(y_batch, preds, average="macro", zero_division=1)

    acc_sl_nb.append(acc)
    batch_metrics.append({
        "Batch": i + 1,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    print(f"📦 Batch {i+1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}")

    recent_X.append(X_batch)
    recent_y.append(y_batch)

    # Drift detection using static accuracy threshold
    if acc < drift_threshold:
        print(f"⚠️ Drift detected at batch {i+1}! Retraining the model...")
        X_retrain = np.vstack([train_scaled, *recent_X[-retrain_window:]])
        y_retrain = np.hstack([train_y, *recent_y[-retrain_window:]])
        clf.fit(X_retrain, y_retrain)
        print(f"🔁 Model retrained on last {retrain_window} batches.")

# Final summary
results_df = pd.DataFrame(batch_metrics)

print("\n📊 Average Metrics Across All Batches (Naive Bayes with Accuracy Threshold Drift Handling):")
print(results_df[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

"""

---

# **DRIFT USING ADWIN**


---

"""

!pip install river

from river.drift import ADWIN

detector = ADWIN()

window_accuracy = []
buffer_X = []
buffer_y = []
retrain_size = 300

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000)
model.fit(train_scaled, train_y)


for i in range(len(test_scaled)):
    x = pd.DataFrame([test_scaled[i]], columns=train.columns)
    y_true = test_y[i]
    print("Before prediction", i)
    y_pred = model.predict(x)[0]
    print("After prediction", i)

    correct = int(y_pred == y_true)
    drift_detected = detector.update(1 - correct)

    # Accumulate for potential retraining
    buffer_X.append(test_scaled[i])
    buffer_y.append(y_true)

    if drift_detected:
        print(f"\nDrift detected at index {i}. Retraining with last {retrain_size} samples.")
        if len(buffer_X) >= retrain_size:
            recent_X = np.array(buffer_X[-retrain_size:])
            recent_y = np.array(buffer_y[-retrain_size:])
            model.fit(recent_X, recent_y)
            print("Model retrained.")
        detector = ADWIN()  # Reset detector after retraining

    window_accuracy.append(correct)
print(f"\nFinal Accuracy after streaming: {np.mean(window_accuracy):.4f}")

detector = ADWIN()

# Streaming and drift detection
window_accuracy = []
buffer_X = []
buffer_y = []
retrain_size = 300

model = rf_classifier

for i in range(len(test_scaled)):
    x = pd.DataFrame([test_scaled[i]], columns=train.columns)
    y_true = test_y[i]
    y_pred = model.predict(x)[0]

    correct = int(y_pred == y_true)
    drift_detected = detector.update(1 - correct)

    # Accumulate for potential retraining
    buffer_X.append(test_scaled[i])
    buffer_y.append(y_true)

    if drift_detected:
        print(f"\nDrift detected at index {i}. Retraining with last {retrain_size} samples.")
        if len(buffer_X) >= retrain_size:
            recent_X = np.array(buffer_X[-retrain_size:])
            recent_y = np.array(buffer_y[-retrain_size:])
            model.fit(recent_X, recent_y)
            print("Model retrained.")
        detector = ADWIN()  # Reset detector after retraining

    window_accuracy.append(correct)
print(f"\nFinal Accuracy after streaming: {np.mean(window_accuracy):.4f}")

# Assuming 'predictions' and 'test_y' are your model predictions and true labels
adwin = ADWIN()
for i in range(len(predictions)):
    adwin.add_element(predictions[i] == test_y[i])
    if adwin.detected_change():
        print(f"Change detected at index {i}")
        # Here you would retrain your model or take other corrective actions

# Example of retraining the model after drift is detected
# (This is just a basic example, adjust as needed)

if adwin.detected_change():
  print("Retraining the model...")
  # Split the data based on drift detection point.
  retrain_data = train.iloc[i:]
  retrain_labels = train_y[i:]
  rf_classifier.fit(retrain_data, retrain_labels) # Retrain RandomForest
  predictions_retrained = rf_classifier.predict(test)  # Make predictions with the retrained model
  xgb_model.fit(retrain_data, retrain_labels) # Retrain XGBoost
  xgb_predictions_retrained = xgb_model.predict(test) # Make predictions with the retrained model

  print(classification_report(test_y, predictions_retrained, target_names=le_target.classes_))
  accuracy = accuracy_score(test_y, predictions_retrained)
  print(f"Accuracy after retraining RandomForest: {accuracy:.4f}")
  print(classification_report(test_y, xgb_predictions_retrained, target_names=le_target.classes_, zero_division=1))
  accuracy = accuracy_score(test_y, xgb_predictions_retrained)
  print(f"Accuracy after retraining XGBoost: {accuracy:.4f}")

!pip install scikit-multiflow

"""

---

# New Section

---

"""

# KNN

from sklearn.neighbors import KNeighborsClassifier

# Initialize and train the KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors
knn_classifier.fit(train, train_y)

# Make predictions on the test set
knn_predictions = knn_classifier.predict(test)

# Evaluate the model
accuracy = accuracy_score(test_y, knn_predictions)
print(f"KNN Accuracy: {accuracy:.4f}")
print(classification_report(test_y, knn_predictions, target_names=le_target.classes_))

"""

---


# **DRIFT Using KL Dvergence**

---

"""

import numpy as np
import pandas as pd
from scipy.stats import entropy
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# --------- KL Divergence Function ---------
def kl_divergence(p, q):
    p = np.histogram(p, bins=50, density=True)[0] + 1e-6
    q = np.histogram(q, bins=50, density=True)[0] + 1e-6
    return entropy(p, q)

# --------- Parameters ---------
threshold = 10.0  # <-- changed from 5.0 for max_kl-based detection
batch_size = 2500
scaler = StandardScaler()

# --------- Fit scaler ONLY on train ---------
train_scaled = scaler.fit_transform(train)

# Train initial model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(train_scaled, train_y)

# Ensure test_y is a Series
if isinstance(test_y, np.ndarray):
    test_y = pd.Series(test_y)

batch_results = []
acc_kl = []

# --------- Main Loop ---------
for batch_start in range(0, test.shape[0], batch_size):
    batch = test.iloc[batch_start:batch_start + batch_size].copy()
    batch_y = test_y.iloc[batch_start:batch_start + batch_size].copy().to_numpy()

    # Transform batch using pre-fit scaler
    batch_scaled = scaler.transform(batch)

    # --------- KL Divergence Drift Detection ---------
    kl_values = [kl_divergence(train_scaled[:, i], batch_scaled[:, i]) for i in range(train_scaled.shape[1])]
    mean_kl = np.mean(kl_values)
    max_kl = np.max(kl_values)

    # --------- Evaluate Pretrained Model ---------
    predictions = model.predict(batch_scaled)
    acc = accuracy_score(batch_y, predictions)
    prec = precision_score(batch_y, predictions, average="macro", zero_division=1)
    rec = recall_score(batch_y, predictions, average="macro", zero_division=1)
    f1 = f1_score(batch_y, predictions, average="macro", zero_division=1)

    # --------- Drift Detection based on max_kl ---------
    is_drift = max_kl > threshold
    acc_kl.append(acc)

    batch_results.append({
        "Batch": batch_start // batch_size + 1,
        "Model": "Random Forest",
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1,
        "Drift": is_drift
    })

    print(f"Batch {batch_start // batch_size + 1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f} | Drift: {'Yes' if is_drift else 'No'} | Mean KL: {mean_kl:.2f} | Max KL: {max_kl:.2f}")

    # --------- Retrain if Drift Detected ---------
    if is_drift:
        unique_classes, class_counts = np.unique(batch_y, return_counts=True)
        if len(unique_classes) > 1 and min(class_counts) > 5:
            smote = SMOTE(k_neighbors=min(5, min(class_counts) - 1))
            batch_resampled, batch_y_resampled = smote.fit_resample(batch, batch_y)
            batch_scaled_resampled = scaler.fit_transform(batch_resampled)
            model.fit(batch_scaled_resampled, batch_y_resampled)
            print(f"🔁 Model retrained on batch {batch_start // batch_size + 1}")
        else:
            print(f"⚠️ Skipping SMOTE in batch {batch_start // batch_size + 1} due to class imbalance.")

# --------- Summary ---------
results_df = pd.DataFrame(batch_results)

print("\n📊 Average Metrics Across All Batches:")
print(results_df.groupby("Model")[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

print("\n📈 Drift Summary:")
print(results_df["Drift"].value_counts())

import numpy as np
import pandas as pd
from scipy.stats import entropy
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# --------- KL Divergence Function ---------
def kl_divergence(p, q):
    p = np.histogram(p, bins=50, density=True)[0] + 1e-6
    q = np.histogram(q, bins=50, density=True)[0] + 1e-6
    return entropy(p, q)

# --------- Parameters ---------
threshold = 18.0
batch_size = 2500
retrain_window = 5
scaler = StandardScaler()

# --------- Fit scaler ONLY on train ---------
train_scaled = scaler.fit_transform(train)

# Train initial Naive Bayes model
model = GaussianNB()
model.fit(train_scaled, train_y)

# Ensure test_y is a Series
if isinstance(test_y, np.ndarray):
    test_y = pd.Series(test_y)

batch_results = []
acc_kl_nb = []
recent_X = []
recent_y = []

# --------- Main Loop ---------
for batch_start in range(0, test.shape[0], batch_size):
    batch = test.iloc[batch_start:batch_start + batch_size].copy()
    batch_y = test_y.iloc[batch_start:batch_start + batch_size].copy().to_numpy()

    # Transform batch using pre-fit scaler
    batch_scaled = scaler.transform(batch)

    # --------- KL Divergence Drift Detection ---------
    kl_values = [kl_divergence(train_scaled[:, i], batch_scaled[:, i]) for i in range(train_scaled.shape[1])]
    mean_kl = np.mean(kl_values)
    max_kl = np.max(kl_values)

    # --------- Evaluate Pretrained Model ---------
    predictions = model.predict(batch_scaled)
    acc = accuracy_score(batch_y, predictions)
    prec = precision_score(batch_y, predictions, average="macro", zero_division=1)
    rec = recall_score(batch_y, predictions, average="macro", zero_division=1)
    f1 = f1_score(batch_y, predictions, average="macro", zero_division=1)

    is_drift = max_kl > threshold
    acc_kl_nb.append(acc)

    batch_results.append({
        "Batch": batch_start // batch_size + 1,
        "Model": "Naive Bayes",
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1,
        "Drift": is_drift
    })

    print(f"📦 Batch {batch_start // batch_size + 1} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f} | Drift: {'Yes' if is_drift else 'No'} | Mean KL: {mean_kl:.2f} | Max KL: {max_kl:.2f}")

    # Store recent batches
    recent_X.append(batch)
    recent_y.append(batch_y)
    recent_X = recent_X[-retrain_window:]
    recent_y = recent_y[-retrain_window:]

    # --------- Retrain if Drift Detected ---------
    if is_drift:
        X_retrain = np.vstack([train] + recent_X)
        y_retrain = np.hstack([train_y] + recent_y)

        unique_classes, class_counts = np.unique(y_retrain, return_counts=True)
        if len(unique_classes) > 1 and min(class_counts) > 5:
            smote = SMOTE(k_neighbors=min(5, min(class_counts) - 1))
            X_resampled, y_resampled = smote.fit_resample(X_retrain, y_retrain)
            X_resampled_scaled = scaler.fit_transform(X_resampled)
            model.fit(X_resampled_scaled, y_resampled)
            print(f"🔁 Model retrained on combined train + last {retrain_window} batches.")
        else:
            print(f"⚠️ Skipping SMOTE due to class imbalance.")

# --------- Summary ---------
results_df = pd.DataFrame(batch_results)

print("\n📊 Average Metrics Across All Batches (Naive Bayes + KL Drift):")
print(results_df.groupby("Model")[["Accuracy", "Precision", "Recall", "F1-score"]].mean().round(4))

print("\n📈 Drift Summary:")
print(results_df["Drift"].value_counts())

"""

---

**Deep Learning Models**

less accuracy on this dataset


---

"""

# CNN ::

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow import keras
from tensorflow.keras import layers

# Assuming the data preprocessing from the previous code is already done and
# 'train', 'test', 'train_y', 'test_y' are available.

# Reshape the data for CNN input (assuming 1 channel)
train = np.expand_dims(train, axis=2)
test = np.expand_dims(test, axis=2)

# Define the CNN model
model = keras.Sequential([
    layers.Conv1D(32, 3, activation='relu', input_shape=train.shape[1:]),
    layers.MaxPooling1D(2),
    layers.Conv1D(64, 3, activation='relu'),
    layers.MaxPooling1D(2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(len(np.unique(train_y)), activation='softmax') # Output layer with softmax
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy', # Use sparse categorical crossentropy for integer labels
              metrics=['accuracy'])

# Train the model
model.fit(train, train_y, epochs=10, batch_size=32, validation_split=0.2) # Adjust epochs and batch size

# Evaluate the model
loss, accuracy = model.evaluate(test, test_y)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")


# Make predictions
predictions = np.argmax(model.predict(test), axis=-1)

# You can then use classification_report and other metrics as before
from sklearn.metrics import classification_report
print(classification_report(test_y, predictions, target_names=le_target.classes_))

# RNN ::

# Reshape the data for RNN input
timesteps = 1  # You can experiment with different timestep values
train_rnn = train.reshape(-1, timesteps, train.shape[2]) # remove .values
test_rnn = test.reshape(-1, timesteps, test.shape[2]) # remove .values


# Define the RNN model
model = keras.Sequential([
    layers.LSTM(64, input_shape=(train_rnn.shape[1], train_rnn.shape[2])),
    layers.Dense(len(np.unique(train_y)), activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(train_rnn, train_y, epochs=20, batch_size=32, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(test_rnn, test_y)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Make predictions
predictions = np.argmax(model.predict(test_rnn), axis=-1)

# Print classification report
print(classification_report(test_y, predictions, target_names=le_target.classes_))

import matplotlib.pyplot as plt
import numpy as np

# Methods and metrics
methods = ['Without Drift Handling', 'KL-Divergence', 'Accuracy-Based Window', 'EDDM']
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']

# Values (as given)
values = [
    [0.8468, 0.8301, 0.6719, 0.6290],  # No Drift Handling
    [0.7644, 0.6476, 0.6122, 0.5849],  # KL-Divergence
    [0.8600, 0.8136, 0.6790, 0.6458],  # Sliding Window
    [0.8632, 0.8057, 0.6752, 0.6412]   # EDDM
]

# Transpose for plotting (metric-wise grouping)
values = np.array(values).T

# Plotting
x = np.arange(len(metrics))  # Metric categories
bar_width = 0.2

plt.figure(figsize=(10, 6))

for i in range(len(methods)):
    plt.bar(x + i * bar_width, values[:, i], width=bar_width, label=methods[i])

# Labels and formatting
plt.xlabel('Evaluation Metric')
plt.ylabel('Score')
plt.title('Comparison of Drift Detection Methods')
plt.xticks(x + bar_width * 1.5, metrics)
plt.ylim(0, 1)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Methods and metrics
methods = ['Without Drift Handling', 'KL-Divergence', 'Accuracy-Based Window', 'EDDM']
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']

# Values (as given)
values = [
    [0.4437, 0.3737, 0.4249, 0.1034],  # No Drift Handling
    [0.6428, 0.4901, 0.6509, 0.4240],  # KL-Divergence
    [0.6366, 0.5123, 0.6027, 0.4210],  # Sliding Window
    [0.6387, 0.5036, 0.5984, 0.4229]   # EDDM
]

# Transpose for plotting (metric-wise grouping)
values = np.array(values).T

# Plotting
x = np.arange(len(metrics))  # Metric categories
bar_width = 0.2

plt.figure(figsize=(10, 6))

for i in range(len(methods)):
    plt.bar(x + i * bar_width, values[:, i], width=bar_width, label=methods[i])

# Labels and formatting
plt.xlabel('Evaluation Metric')
plt.ylabel('Score')
plt.title('Comparison of Drift Detection Methods')
plt.xticks(x + bar_width * 1.5, metrics)
plt.ylim(0, 1)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Example: Batch-wise accuracy values (replace with your real values)
    # Add full list here

batches = list(range(1, len(acc_no_drift) + 1))

plt.figure(figsize=(10, 5))
plt.plot(batches, acc_no_drift, marker='o', linestyle='-', label='Without Drift Handling')
plt.plot(batches, acc_kl, marker='s', linestyle='--', label='KL-Divergence')

plt.title('Accuracy per Batch: Without Drift vs KL-Divergence')
plt.xlabel('Batch Number')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Example: Batch-wise accuracy values (replace with your real values)
    # Add full list here

batches = list(range(1, len(acc_no_drift) + 1))

plt.figure(figsize=(10, 5))
plt.plot(batches, acc_no_drift, marker='o', linestyle='-', label='Without Drift Handling')
plt.plot(batches, acc_sl, marker='s', linestyle='--', label='Sliding Window')

plt.title('Accuracy per Batch: Without Drift vs Accuracy based sliding window')
plt.xlabel('Batch Number')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Example: Batch-wise accuracy values (replace with your real values)
    # Add full list here

batches = list(range(1, len(acc_no_drift) + 1))

plt.figure(figsize=(10, 5))
plt.plot(batches, acc_no_drift, marker='o', linestyle='-', label='Without Drift Handling')
plt.plot(batches, acc_eddm, marker='s', linestyle='--', label='EDDM')

plt.title('Accuracy per Batch: Without Drift vs EDDM')
plt.xlabel('Batch Number')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Example: Batch-wise accuracy values (replace with your real values)
    # Add full list here

batches = list(range(1, len(acc_no_drift) + 1))

plt.figure(figsize=(10, 5))
plt.plot(batches, acc_no_drift, marker='o', linestyle='-', label='Without Drift Handling')
plt.plot(batches, acc_kl_nb, marker='s', linestyle='--', label='KL-Divergence')

plt.title('Accuracy per Batch: Without Drift vs KL-Divergence')
plt.xlabel('Batch Number')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend()
plt.tight_layout()
plt.show()